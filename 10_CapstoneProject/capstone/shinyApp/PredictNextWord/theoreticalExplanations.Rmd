---
title: "Untitled"
date: "Saturday, August 02, 2014"
output:
  html_document:
    mathjax: "//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
runtime: shiny
---

## Theoretical background behind the applied NLP models

To model the probability of a word given a sentence or a given sequence of words, the accurate solution would be to count each combination of that sentence with a following word and divide it by the count of the given sentence [1]. Counting all of these combination is far too computationally costly, and also would require a specific amount of statistics on all combinations, that might not be given in a training data set.
Therefore the probabilites can be estimated based on the <b>Markov assumption</b>, which is a simplifying assumption that states that the probability of a word given a sequence can be estimated as the probability of a word given only the previous word, or a specifc ngram of previous words.
Formally, the Markov assumption says the probability of a sequence of words is the product of the conditional probability of the to be predicted word given some prefix of the last few words of the input sequence: $$P(w_\text{1}w_\text{2} ... w_n)\approx \prod_{i}P(w_{i}|w_{i-k} ... w_{i-1}) $$ where $k$ is the amount of last words of the considered sequence.

In other words, the components of the conditional products are estimated as the conditional product of only the word of interest with only the last k words instead of all given words of the sequence: $$P(w_i | w_\text{1}w_\text{2} ... w_n)\approx P(w_{i}|w_{i-k} ... w_{i-1}) \quad\text{.}$$

The simplest case of a Markov model is the <b>unigram model</b> [1]. Here, the probability of a sequence of words is estimated by the product of the probability of the individual words or unigrams: $$P(w_\text{1}w_\text{2} ... w_n)\approx \prod_{i}P(w_{i}) $$ where words are considered independent.

Slightly more advanced is a <b>bigram model</b> where a condition is set on the given single previous words. Hence here, the probability of a word given the entire given prefix of a word is estimated by the probability of the word given only the previous word: $$P(w_i | w_\text{1}w_\text{2} ... w_{}i-1)\approx P(w_{i} | w_{i-1}) \quad\text{.}$$

The Ngram models can be advanced using 3grams, 4grams etc. Generally, language can have long-distance word dependencies which leads to insuffienct performance of these simple Ngram models, yet it was found that normally these simplistic Ngram models perform satisfacotrily.

Generally, the probabilities of any Ngram model can be calucated by the division of the overall count of the ngram divided by the count of the single word of interest: $$ P(w_i | w_{i-1}) = \frac{{count}(w_{i-1},w_i) }{{count}(w_{i-1})}$$

Depending on the size of the training data set, there might be cases where the probability of an ngram is zero, just because it was not available in the data set beforehand. When calculating a sequence of words, or a sentence, by the product of all ngrams in that sentence, this will lead to a probability of 0 for that sentence. This does not seem right, or representative. To avoid that, the probability of that sentence can be caluclated by the sum of the $log$s of the individual probabilites (instead of the product of the individual probabilities), or by applying <b>smoothing techniques</b>. Smoothing prefents the outcome of zero counts of a word, leading to the ability of calculating the overall probability of a sentence. The simplest applicaiton of smoothing is <b>add-one smoothing</b> (or Laplace smoothing) [1], where all words start with a base count of 1. This leads to the conditional probability estimation of each Ngram of (here as example of bigrams): $$ P_\text{Add-1}(w_i | w_{i-1}) = \frac{{count}(w_{i-1},w_i)+1 }{{count}(w_{i-1}) + V}$$ where V is the sum of all added 1s of the individual words in the vocabulary.

In case of relatively low counts of ngrams, the add-1 smoothing can have a drastic impact on the probabilities that leads to big changes of the actual probabilites. This is why, add-one estimation is not generally used for n-gram models, and other advanced smooting techniques like e.g. <b>Kneser-Ney smoothing</b> are applied, as explained further down.

In our problem at hand, we need to calculate the probability of a word given a specific input sequence of words. Intuitively, one would calculate the largest Ngram possible (computationally) for that decision. In some cases, the large Ngram does not exist, or has a probability of 0, which is when <b>backoff</b> or <b>interpolation</b> techniques [1] can be applied. The former technique just means, that if you have no or bad evidence for a specific Ngram probability, just move or backoff to the next smaller (N-1)gram instead where a more accurate probability is given. This can be applied in many steps up until you take the simple Unigram probability. Interpolation on the other hand, takes into account a mix of all Ngrms to calculate the probability of the next word. This can be applied by a linear combination of the individual probabilities weighting them individually by a pre-factor $\lambda_i$. A combination of both backoff and interpolation is the <b>stupid backoff</b> where the probability for a conditional probability of an ngram is calculated either directly, or if the count of the bigram is zero, by the probability of the (n-1)gram times a specific weight $\lambda$=0.4 .

The add-1 smoothing explained earlier can be addapted to <b>add-k smoothing</b> where instead of 1 a value k is added in the nominator, and hence $kV$ in the denominator. This is the principle of further advanced models which take probabilites of the next lower (n-k)grams as cummulative weighting factors for the higher ngrams rather than constant summands. Hence you include probabilities of words in your estimators that you have seen at least once, in order to help estimate the one that you have not seen.

In <b>Kneser-Ney smoothing</b> [2-4] the probability of word following a specific ngram is calculated according to the history of the word given a lower order (n-k)gram of that word [2]. For a bigram the propabilities can be exemplarily calculated as follows:

$$ P_\text{KN} (w_i) = \frac{{max}(c(w_{i-1},w_i) - D)}{\sum_{\widetilde{w}} c(w_{i-1},\widetilde{w})} + \lambda_{w_{i-1}} P_{KN}(w_i) $$ where $P_{KN}(w_i)$ is the probability of a unigram or individual token calculated from a non-zero count of the corpus. The parameter $D$ is an absolute constant which denotes the discount value substracted from the count of each n-gram which weakens n-grams with lower frequencies to eventually omit them. The discounting technique is specifically applied in Kneser-Ney smoothing and for that is considered the most effective mehtod of smoothing when applying ngram models. $\lambda_{w_{i-1}}$ is a weighting factor of the probability of the unigram probability. For higher order ngrams, the Kneser-Ney propabilities are calculated recursively by adding always the weighted probability of the next-lower ngram in the calculation: $$ P_\text{KN} (w_i | w^{i-1}_{i-n+1}) = \frac{{max}(c_{KN}(w^{i}_{i-n+1}) - D)}{c_{KN}(w^{i-1}_{i-n+1})} + \lambda(w^{i-1}_{i-n+1}) P_{KN}(w_i | w^{i-1}_{i-n+2}) \quad\text{.} $$ The discount constant can be calculated as follows [4]: $$D_{k,i} = i - (i+1) Y_k \frac{N_{k,i+1}}{N_{k,i}} $$ where $Y_k = \frac{N_{k,1}}{N_{k,1}+2N_{k,2}}$. The notation $N_i$ means the number of ngrams with $i$ occurrences, in other words the frequency of ngram frequency $i$.

In the application built for this capstone project, we compare a simple <b>backoff</b> smoothing with a <b>Kneser-Ney</b> smoothing in both cases up to ngrams of the order of $n\leq 4$. In the application one can switch between both models and compare the outcomes.

## References
[1] Opencourseonline.com, Stanford NLP, Professor Dan Jurafsky & Chris Manning, Videos 4-1 to 4-8

[2] Opencourseonline.com, Stanford NLP, Professor Dan Jurafsky & Chris Manning, Video 4-8 (https://www.youtube.com/watch?v=wtB00EczoCM&t=36s)

[3] wikipedia, https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing

[4] Smitha Milli blog post about Kneser-Ney Smoothing, http://smithamilli.com/blog/kneser-ney/